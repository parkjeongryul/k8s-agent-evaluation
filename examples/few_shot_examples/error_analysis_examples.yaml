category: error_analysis
description: "전문가가 작성한 K8s 오류 분석 우수 응답 예시"
created_by: "K8s 전문가"
last_updated: "2024-01-20"

examples:
  - id: "ea_001"
    query:
      type: "error_analysis"
      user_query: "Pod가 CrashLoopBackOff 상태이고 'exec /docker-entrypoint.sh: no such file or directory' 에러가 발생합니다."
      context:
        error_message: "exec /docker-entrypoint.sh: no such file or directory"
        pod_status: "CrashLoopBackOff"
        container_image: "nginx:alpine"
    
    expert_response: |
      이 오류는 컨테이너 이미지에서 지정된 엔트리포인트 파일을 찾을 수 없어 발생합니다.
      
      **원인 분석:**
      1. 잘못된 베이스 이미지 사용 (nginx:alpine vs nginx)
      2. COPY 명령어에서 엔트리포인트 파일 누락
      3. 파일 권한 문제 (실행 권한 없음)
      
      **해결 방법:**
      1. Dockerfile 확인: `COPY docker-entrypoint.sh /docker-entrypoint.sh`
      2. 실행 권한 추가: `RUN chmod +x /docker-entrypoint.sh`
      3. 또는 기본 nginx 이미지 사용: `FROM nginx` (alpine 대신)
      
      **검증:**
      ```bash
      kubectl describe pod <pod-name>  # 이벤트 확인
      kubectl logs <pod-name> --previous  # 이전 실행 로그 확인
      ```
    
    expert_reasoning: |
      CrashLoopBackOff와 "no such file or directory" 조합은 거의 항상 컨테이너 빌드 문제입니다.
      특히 alpine 베이스는 경량화로 인해 일부 파일이 누락될 수 있어 주의가 필요합니다.
      해결책을 제시할 때는 즉시 적용 가능한 방법과 근본 원인 해결을 모두 포함해야 합니다.
    
    key_points:
      - "엔트리포인트 파일 누락 식별"
      - "Dockerfile 검토 필요성"
      - "파일 권한 확인"
      - "베이스 이미지 차이점 설명"
      - "검증 명령어 제공"
    
    quality_score: 0.95

  - id: "ea_002"
    query:
      type: "error_analysis"
      user_query: "OOMKilled 에러로 포드가 재시작되는데, 메모리 사용량이 limit보다 낮아 보입니다."
      context:
        error: "OOMKilled"
        memory_limit: "1Gi"
        observed_usage: "800Mi"
    
    expert_response: |
      메모리 사용량이 limit보다 낮아 보여도 OOMKilled가 발생하는 경우의 원인들:
      
      **가능한 원인:**
      1. **메모리 스파이크**: 순간적인 메모리 사용량 급증
      2. **캐시/버퍼 메모리**: OS 레벨 캐시가 포함되지 않은 측정
      3. **메모리 파편화**: 연속된 메모리 블록 부족
      4. **JVM 힙 설정**: -Xmx가 컨테이너 limit과 불일치
      
      **정밀 분석:**
      ```bash
      # 실시간 메모리 사용량 모니터링
      kubectl top pod <pod-name> --containers
      
      # cgroup 메모리 통계 확인 (더 정확)
      kubectl exec <pod-name> -- cat /sys/fs/cgroup/memory/memory.usage_in_bytes
      kubectl exec <pod-name> -- cat /sys/fs/cgroup/memory/memory.limit_in_bytes
      ```
      
      **해결 방안:**
      1. 메모리 limit을 1.5Gi로 증가 (여유분 확보)
      2. JVM 애플리케이션의 경우: `-Xmx800m` 설정
      3. 메모리 사용 패턴 모니터링 도구 도입 (Prometheus + Grafana)
      
      **예방책:**
      - 메모리 request를 limit의 80% 수준으로 설정
      - 애플리케이션 레벨 메모리 최적화
    
    expert_reasoning: |
      OOMKilled는 커널 레벨에서 발생하므로 kubectl top의 메트릭과 실제 커널이 보는 메모리 사용량에 차이가 있을 수 있습니다.
      특히 Java 애플리케이션은 힙 외부 메모리(메타스페이스, 직접 메모리)도 고려해야 합니다.
      단순히 limit만 늘리는 것보다 근본 원인을 파악하는 것이 중요합니다.
    
    key_points:
      - "메모리 스파이크 현상 설명"
      - "cgroup 메모리 통계 활용"
      - "JVM 힙 설정 고려사항"
      - "정밀 모니터링 방법"
      - "예방적 리소스 설정"
    
    quality_score: 0.92