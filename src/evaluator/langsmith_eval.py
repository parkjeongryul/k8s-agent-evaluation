import asyncio
from typing import Dict, Any, List, Optional
from uuid import uuid4
from datetime import datetime

# LangSmith í‰ê°€ í”„ë ˆì„ì›Œí¬ (ì™¸ë¶€ ì „ì†¡ ì—†ìŒ)
from langsmith.evaluation import evaluate, LangSmithEvaluator as BaseLangSmithEvaluator
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

from .base import BaseEvaluator
from ..data.schemas import (
    K8sQuery, AgentResponse, GroundTruth, 
    EvaluationResult, TestCase
)


class EvaluationOutput(BaseModel):
    correctness_score: float = Field(..., ge=0.0, le=1.0)
    relevance_score: float = Field(..., ge=0.0, le=1.0)
    completeness_score: float = Field(..., ge=0.0, le=1.0)
    reasoning: str = Field(..., description="í‰ê°€ ê·¼ê±°")
    few_shot_comparison: str = Field(..., description="ì „ë¬¸ê°€ ì˜ˆì‹œì™€ì˜ ë¹„êµ")
    missing_points: List[str] = Field(default_factory=list)


class LangSmithEvaluator(BaseEvaluator):
    """LangSmith í‰ê°€ì - Few-shot + LLM í‰ê°€ (ì™¸ë¶€ ì „ì†¡ ì°¨ë‹¨)"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        
        # ğŸ”’ ì™¸ë¶€ ì „ì†¡ ì°¨ë‹¨ í™•ì¸
        import os
        if os.getenv('LANGCHAIN_API_KEY') or os.getenv('LANGCHAIN_TRACING_V2'):
            print("ğŸš« LangSmith ì™¸ë¶€ ì „ì†¡ í™˜ê²½ë³€ìˆ˜ ê°ì§€ë¨ - ì°¨ë‹¨ë¨")
            os.environ.pop('LANGCHAIN_API_KEY', None)
            os.environ.pop('LANGCHAIN_TRACING_V2', None)
            os.environ.pop('LANGCHAIN_PROJECT', None)
        
        print("ğŸ”’ LangSmith í”„ë ˆì„ì›Œí¬ë§Œ ì‚¬ìš© (ì™¸ë¶€ ì „ì†¡ ì™„ì „ ì°¨ë‹¨)")
        
        # LLM ì´ˆê¸°í™” (ì‚¬ë‚´ ì„œë²„)
        self.llm = ChatOpenAI(
            model=config.get("model", "gpt-4-turbo-preview"),
            temperature=config.get("temperature", 0.0)
        )
        
        # ì¶œë ¥ íŒŒì„œ
        self.parser = PydanticOutputParser(pydantic_object=EvaluationOutput)
        
        # Few-shot ì˜ˆì œ ë¡œë“œ
        self.few_shot_dir = config.get("few_shot_dir", "examples/few_shot_examples")
        self.few_shot_examples = self._load_few_shot_examples()
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        self.evaluation_prompt = self._create_evaluation_prompt()
        
    def _load_few_shot_examples(self) -> Dict[str, List[Dict]]:
        """ë¡œì»¬ YAML íŒŒì¼ì—ì„œ Few-shot ì˜ˆì œ ë¡œë“œ"""
        import yaml
        from pathlib import Path
        
        examples = {}
        few_shot_path = Path(self.few_shot_dir)
        
        if not few_shot_path.exists():
            print(f"âš ï¸ Few-shot ë””ë ‰í† ë¦¬ {few_shot_path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return examples
            
        for yaml_file in few_shot_path.glob("*.yaml"):
            try:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                category = data.get('category')
                if category:
                    examples[category] = data.get('examples', [])
                    print(f"âœ… {category} ì „ë¬¸ê°€ ì˜ˆì œ {len(data.get('examples', []))}ê°œ ë¡œë“œ")
            except Exception as e:
                print(f"âŒ {yaml_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return examples
    
    def _create_evaluation_prompt(self) -> ChatPromptTemplate:
        """LangSmith ê¸°ë°˜ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ K8s ì „ë¬¸ê°€ì…ë‹ˆë‹¤. Agentì˜ ì‘ë‹µì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:

1. **ì •í™•ì„± (Correctness)**: ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ì œ ì ìš© ê°€ëŠ¥í•œê°€?
2. **ê´€ë ¨ì„± (Relevance)**: ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ”ê°€?
3. **ì™„ì „ì„± (Completeness)**: ë¬¸ì œ í•´ê²°ì— í•„ìš”í•œ ëª¨ë“  ìš”ì†Œë¥¼ ë‹¤ë£¨ëŠ”ê°€?

ì•„ë˜ ì „ë¬¸ê°€ ì˜ˆì‹œë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”. ë™ì¼í•œ í’ˆì§ˆ ìˆ˜ì¤€ì„ ìš”êµ¬í•©ë‹ˆë‹¤.

{few_shot_examples}

{format_instructions}"""),
            
            ("human", """**ì‚¬ìš©ì ì§ˆë¬¸:**
{user_query}

**ë§¥ë½ ì •ë³´:**
{context}

**Agent ì‘ë‹µ:**
{agent_response}

**ê¸°ëŒ€ ë‹µë³€:**
{expected_answer}

**í•µì‹¬ í¬ì¸íŠ¸:**
{key_points}

ìœ„ ì „ë¬¸ê°€ ì˜ˆì‹œ ìˆ˜ì¤€ìœ¼ë¡œ Agent ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”.""")
        ])
    
    def get_relevant_examples(
        self, 
        query_type: str, 
        limit: int = 2
    ) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ íƒ€ì…ì— ë§ëŠ” Few-shot ì˜ˆì œ ë°˜í™˜"""
        type_mapping = {
            "error_analysis": "error_analysis",
            "performance": "performance",
            "configuration": "configuration",
            "scaling": "scaling",
            "troubleshooting": "troubleshooting"
        }
        
        category = type_mapping.get(query_type)
        if not category or category not in self.few_shot_examples:
            return []
        
        examples = self.few_shot_examples[category]
        
        # í’ˆì§ˆ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ì˜ˆì œ ì„ íƒ
        sorted_examples = sorted(
            examples,
            key=lambda x: x.get('quality_score', 0),
            reverse=True
        )
        
        return sorted_examples[:limit]
    
    def _format_few_shot_examples(self, examples: List[Dict[str, Any]]) -> str:
        """Few-shot ì˜ˆì œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not examples:
            return "ê´€ë ¨ ì „ë¬¸ê°€ ì˜ˆì‹œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_examples = []
        for i, example in enumerate(examples, 1):
            query_info = example.get('query', {})
            expert_response = example.get('expert_response', '')
            expert_reasoning = example.get('expert_reasoning', '')
            key_points = example.get('key_points', [])
            
            example_text = f"""
## ì „ë¬¸ê°€ ì˜ˆì‹œ {i}

**ì§ˆë¬¸:** {query_info.get('user_query', '')}

**ì „ë¬¸ê°€ ì‘ë‹µ:**
{expert_response}

**ì „ë¬¸ê°€ ì¶”ë¡ :**
{expert_reasoning}

**í•µì‹¬ í¬ì¸íŠ¸:** {', '.join(key_points)}

---"""
            formatted_examples.append(example_text)
        
        return "\n".join(formatted_examples)
    
    async def evaluate_single(
        self, 
        query: K8sQuery, 
        response: AgentResponse, 
        ground_truth: GroundTruth
    ) -> EvaluationResult:
        """í†µí•© LangSmith í‰ê°€ (ì™¸ë¶€ ì „ì†¡ ì—†ìŒ)"""
        
        # ê´€ë ¨ Few-shot ì˜ˆì œ ê°€ì ¸ì˜¤ê¸°
        query_type_str = query.query_type if isinstance(query.query_type, str) else query.query_type.value
        relevant_examples = self.get_relevant_examples(query_type_str, limit=2)
        few_shot_text = self._format_few_shot_examples(relevant_examples)
        
        # LangSmith í‰ê°€ í•¨ìˆ˜ ì •ì˜
        async def langsmith_evaluator(inputs: dict) -> dict:
            """LangSmith í‰ê°€ í•¨ìˆ˜ (ì‚¬ë‚´ LLM ì‚¬ìš©)"""
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt_value = self.evaluation_prompt.format_prompt(
                few_shot_examples=few_shot_text,
                user_query=inputs["user_query"],
                context=str(inputs.get("context", {})),
                agent_response=inputs["agent_response"],
                expected_answer=inputs["expected_answer"],
                key_points=", ".join(inputs.get("key_points", [])),
                format_instructions=self.parser.get_format_instructions()
            )
            
            # ğŸ”’ ì‚¬ë‚´ LLMìœ¼ë¡œë§Œ í‰ê°€ ì‹¤í–‰
            llm_output = await self.llm.ainvoke(prompt_value.to_messages())
            evaluation = self.parser.parse(llm_output.content)
            
            # LangSmith í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ë°˜í™˜
            overall_score = (
                evaluation.correctness_score * 0.4 +
                evaluation.relevance_score * 0.3 +
                evaluation.completeness_score * 0.3
            )
            
            return {
                "scores": {
                    "correctness": evaluation.correctness_score,
                    "relevance": evaluation.relevance_score,
                    "completeness": evaluation.completeness_score,
                    "overall": overall_score
                },
                "feedback": {
                    "reasoning": evaluation.reasoning,
                    "few_shot_comparison": evaluation.few_shot_comparison,
                    "missing_points": evaluation.missing_points,
                    "few_shot_examples_used": len(relevant_examples)
                }
            }
        
        # í‰ê°€ ì‹¤í–‰ (ì™¸ë¶€ ì „ì†¡ ì—†ìŒ)
        evaluation_input = {
            "user_query": query.user_query,
            "context": query.context,
            "agent_response": response.answer,
            "expected_answer": ground_truth.expected_answer,
            "key_points": ground_truth.key_points
        }
        
        result = await langsmith_evaluator(evaluation_input)
        scores = result["scores"]
        feedback = result["feedback"]
        
        return EvaluationResult(
            evaluation_id=str(uuid4()),
            query_id=query.query_id,
            response_id=response.response_id,
            correctness_score=scores["correctness"],
            relevance_score=scores["relevance"],
            completeness_score=scores["completeness"],
            overall_score=scores["overall"],
            feedback={
                **feedback,
                "evaluation_method": "unified_langsmith_local",
                "external_transmission": "blocked",
                "agent_confidence": response.confidence_score,
                "execution_time": response.execution_time
            }
        )
    
    def calculate_metrics(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """í‰ê°€ ê²°ê³¼ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not results:
            return {}
            
        return {
            "avg_correctness": sum(r.correctness_score for r in results) / len(results),
            "avg_relevance": sum(r.relevance_score for r in results) / len(results),
            "avg_completeness": sum(r.completeness_score for r in results) / len(results),
            "avg_overall": sum(r.overall_score for r in results) / len(results),
            "total_evaluations": len(results),
            "evaluation_method": "unified_langsmith_framework"
        }