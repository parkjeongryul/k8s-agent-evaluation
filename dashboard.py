#!/usr/bin/env python3
"""
í‰ê°€ ê²°ê³¼ë¥¼ ëŒ€ì‹œë³´ë“œ í˜•íƒœë¡œ ë³´ì—¬ì£¼ëŠ” ë„êµ¬
"""
import json
import sys
from pathlib import Path
from datetime import datetime
import glob
import os


class Dashboard:
    """í‰ê°€ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self):
        self.results_dir = "."
        self.all_results = []
    
    def load_all_results(self):
        """ëª¨ë“  í‰ê°€ ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        json_files = glob.glob(os.path.join(self.results_dir, "*evaluation*.json"))
        
        for file in sorted(json_files, reverse=True)[:10]:  # ìµœê·¼ 10ê°œë§Œ
            try:
                with open(file, 'r') as f:
                    data = json.load(f)
                    data['filename'] = os.path.basename(file)
                    self.all_results.append(data)
            except:
                continue
    
    def print_dashboard(self):
        """ëŒ€ì‹œë³´ë“œ ì¶œë ¥"""
        self.clear_screen()
        
        print("â•”" + "â•" * 78 + "â•—")
        print("â•‘" + " K8s Agent í‰ê°€ ëŒ€ì‹œë³´ë“œ ".center(78) + "â•‘")
        print("â• " + "â•" * 78 + "â•£")
        
        if not self.all_results:
            print("â•‘" + " í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ".center(78) + "â•‘")
            print("â•š" + "â•" * 78 + "â•")
            return
        
        # ìµœê·¼ í‰ê°€ ìš”ì•½
        self.print_recent_evaluations()
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ
        self.print_performance_trend()
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥
        self.print_query_type_performance()
        
        print("â•š" + "â•" * 78 + "â•")
    
    def print_recent_evaluations(self):
        """ìµœê·¼ í‰ê°€ ëª©ë¡"""
        print("â•‘ ğŸ“‹ ìµœê·¼ í‰ê°€ ê²°ê³¼" + " " * 61 + "â•‘")
        print("â•‘" + "-" * 78 + "â•‘")
        print("â•‘ ë²ˆí˜¸ â”‚ íŒŒì¼ëª…                          â”‚ Agent â”‚ ì¢…í•©ì ìˆ˜ â”‚ ì‹œê°„     â•‘")
        print("â•‘" + "-" * 78 + "â•‘")
        
        for i, result in enumerate(self.all_results[:5], 1):
            metadata = result.get('evaluation_metadata', result.get('metrics', {}).get('evaluation_metadata', {}))
            metrics = result.get('metrics', {}).get('aggregate', {}).get('overall', {})
            
            filename = result['filename'][:30]
            agent_level = metadata.get('agent_quality_level', 'N/A')[:6]
            overall_score = metrics.get('avg_overall', 0)
            timestamp = metadata.get('timestamp', '')[:10]
            
            score_display = self.format_score(overall_score)
            
            print(f"â•‘  {i:2d}  â”‚ {filename:30} â”‚ {agent_level:6} â”‚ {score_display} â”‚ {timestamp:10} â•‘")
        
        print("â•‘" + "-" * 78 + "â•‘")
    
    def print_performance_trend(self):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ"""
        print("â•‘ ğŸ“ˆ ì„±ëŠ¥ íŠ¸ë Œë“œ (ìµœê·¼ 5ê°œ)" + " " * 52 + "â•‘")
        print("â•‘" + "-" * 78 + "â•‘")
        
        # ì ìˆ˜ ì¶”ì¶œ
        scores = []
        for result in self.all_results[:5]:
            metrics = result.get('metrics', {}).get('aggregate', {}).get('overall', {})
            scores.append(metrics.get('avg_overall', 0))
        
        if scores:
            # ê°„ë‹¨í•œ ì°¨íŠ¸
            max_score = 1.0
            chart_width = 60
            
            for i, score in enumerate(scores):
                bar_length = int(score * chart_width)
                bar = "â–ˆ" * bar_length + "â–‘" * (chart_width - bar_length)
                print(f"â•‘ {i+1} â”‚ {bar} â”‚ {score:.3f} â•‘")
        
        print("â•‘" + "-" * 78 + "â•‘")
    
    def print_query_type_performance(self):
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥"""
        if not self.all_results:
            return
            
        latest = self.all_results[0]
        by_type = latest.get('metrics', {}).get('by_query_type', {})
        
        if by_type:
            print("â•‘ ğŸ” ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥ (ìµœì‹ )" + " " * 51 + "â•‘")
            print("â•‘" + "-" * 78 + "â•‘")
            print("â•‘ íƒ€ì…               â”‚ ì •í™•ì„± â”‚ ê´€ë ¨ì„± â”‚ ì™„ì „ì„± â”‚ ì¢…í•©   â”‚ ê±´ìˆ˜ â•‘")
            print("â•‘" + "-" * 78 + "â•‘")
            
            for query_type, metrics in sorted(by_type.items()):
                print(f"â•‘ {query_type:18} â”‚ "
                      f"{self.format_score_short(metrics.get('avg_correctness', 0))} â”‚ "
                      f"{self.format_score_short(metrics.get('avg_relevance', 0))} â”‚ "
                      f"{self.format_score_short(metrics.get('avg_completeness', 0))} â”‚ "
                      f"{self.format_score_short(metrics.get('avg_overall', 0))} â”‚ "
                      f"{metrics.get('count', 0):4d} â•‘")
            
            print("â•‘" + "-" * 78 + "â•‘")
    
    def format_score(self, score):
        """ì ìˆ˜ í¬ë§·íŒ… (ìƒ‰ìƒ ì—†ì´)"""
        if score >= 0.8:
            return f"{score:6.3f} ğŸŸ¢"
        elif score >= 0.6:
            return f"{score:6.3f} ğŸŸ¡"
        else:
            return f"{score:6.3f} ğŸ”´"
    
    def format_score_short(self, score):
        """ì§§ì€ ì ìˆ˜ í¬ë§·íŒ…"""
        if score >= 0.8:
            return f"{score:.3f}"
        elif score >= 0.6:
            return f"{score:.3f}"
        else:
            return f"{score:.3f}"
    
    def clear_screen(self):
        """í™”ë©´ ì§€ìš°ê¸°"""
        os.system('clear' if os.name == 'posix' else 'cls')
    
    def run_interactive(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        while True:
            self.load_all_results()
            self.print_dashboard()
            
            print("\nì˜µì…˜: [R]ìƒˆë¡œê³ ì¹¨ [1-5]ìƒì„¸ë³´ê¸° [Q]ì¢…ë£Œ")
            choice = input("ì„ íƒ: ").strip().lower()
            
            if choice == 'q':
                break
            elif choice == 'r':
                continue
            elif choice.isdigit() and 1 <= int(choice) <= min(5, len(self.all_results)):
                self.show_details(int(choice) - 1)
            else:
                print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
                input("ê³„ì†í•˜ë ¤ë©´ Enter...")
    
    def show_details(self, index):
        """ìƒì„¸ ì •ë³´ í‘œì‹œ"""
        result = self.all_results[index]
        
        self.clear_screen()
        print("â•”" + "â•" * 78 + "â•—")
        print("â•‘" + f" ìƒì„¸ í‰ê°€ ê²°ê³¼: {result['filename']} ".center(78) + "â•‘")
        print("â• " + "â•" * 78 + "â•£")
        
        # ë©”íƒ€ë°ì´í„°
        metadata = result.get('evaluation_metadata', result.get('metrics', {}).get('evaluation_metadata', {}))
        print("â•‘ ğŸ“‹ í‰ê°€ ì •ë³´" + " " * 65 + "â•‘")
        print("â•‘" + "-" * 78 + "â•‘")
        print(f"â•‘ í‰ê°€ ì‹œê°„: {metadata.get('timestamp', 'N/A'):66} â•‘")
        print(f"â•‘ í‰ê°€ ë°©ì‹: {metadata.get('evaluator_used', 'N/A'):66} â•‘")
        print(f"â•‘ Agent í’ˆì§ˆ: {metadata.get('agent_quality_level', 'N/A'):65} â•‘")
        print(f"â•‘ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {str(metadata.get('total_test_cases', 'N/A')) + 'ê°œ':62} â•‘")
        
        # ì „ì²´ ë©”íŠ¸ë¦­
        metrics = result.get('metrics', {}).get('aggregate', {}).get('overall', {})
        if metrics:
            print("â•‘" + "-" * 78 + "â•‘")
            print("â•‘ ğŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼" + " " * 60 + "â•‘")
            print("â•‘" + "-" * 78 + "â•‘")
            print(f"â•‘ ì¢…í•© ì ìˆ˜: {self.format_score(metrics.get('avg_overall', 0)):>66} â•‘")
            print(f"â•‘ ì •í™•ì„±:   {self.format_score(metrics.get('avg_correctness', 0)):>67} â•‘")
            print(f"â•‘ ê´€ë ¨ì„±:   {self.format_score(metrics.get('avg_relevance', 0)):>67} â•‘")
            print(f"â•‘ ì™„ì „ì„±:   {self.format_score(metrics.get('avg_completeness', 0)):>67} â•‘")
        
        # ê°œì„  ì˜ì—­
        improvement = result.get('metrics', {}).get('improvement_areas', {})
        if improvement:
            print("â•‘" + "-" * 78 + "â•‘")
            print("â•‘ ğŸ¯ ê°œì„  í•„ìš” ì˜ì—­" + " " * 60 + "â•‘")
            print("â•‘" + "-" * 78 + "â•‘")
            print(f"â•‘ ë‚®ì€ ì„±ëŠ¥ ì¼€ì´ìŠ¤: {improvement.get('low_performing_count', 0)}ê°œ ({improvement.get('low_performing_percentage', 0):.1f}%)" + " " * 46 + "â•‘")
            print(f"â•‘ ì£¼ìš” ë¬¸ì œ: {improvement.get('most_common_issue', 'N/A'):65} â•‘")
        
        print("â•š" + "â•" * 78 + "â•")
        
        input("\nê³„ì†í•˜ë ¤ë©´ Enter...")


def compare_results(file1: str, file2: str):
    """ë‘ í‰ê°€ ê²°ê³¼ ë¹„êµ"""
    try:
        with open(file1, 'r') as f:
            data1 = json.load(f)
        with open(file2, 'r') as f:
            data2 = json.load(f)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    print("\n" + "=" * 80)
    print("í‰ê°€ ê²°ê³¼ ë¹„êµ".center(80))
    print("=" * 80)
    
    # ë©”íƒ€ë°ì´í„° ë¹„êµ
    meta1 = data1.get('evaluation_metadata', data1.get('metrics', {}).get('evaluation_metadata', {}))
    meta2 = data2.get('evaluation_metadata', data2.get('metrics', {}).get('evaluation_metadata', {}))
    
    print(f"\n{'':20} {'íŒŒì¼ 1':^25} â”‚ {'íŒŒì¼ 2':^25}")
    print("-" * 80)
    print(f"{'Agent í’ˆì§ˆ':20} {meta1.get('agent_quality_level', 'N/A'):^25} â”‚ {meta2.get('agent_quality_level', 'N/A'):^25}")
    print(f"{'í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤':20} {str(meta1.get('total_test_cases', 'N/A')):^25} â”‚ {str(meta2.get('total_test_cases', 'N/A')):^25}")
    
    # ë©”íŠ¸ë¦­ ë¹„êµ
    metrics1 = data1.get('metrics', {}).get('aggregate', {}).get('overall', {})
    metrics2 = data2.get('metrics', {}).get('aggregate', {}).get('overall', {})
    
    print("\nì „ì²´ ì„±ëŠ¥ ë¹„êµ:")
    print("-" * 80)
    
    for metric in ['avg_overall', 'avg_correctness', 'avg_relevance', 'avg_completeness']:
        val1 = metrics1.get(metric, 0)
        val2 = metrics2.get(metric, 0)
        diff = val2 - val1
        
        metric_name = metric.replace('avg_', '').replace('_', ' ').title()
        
        arrow = "â†‘" if diff > 0 else "â†“" if diff < 0 else "="
        color_diff = f"+{diff:.3f}" if diff > 0 else f"{diff:.3f}"
        
        print(f"{metric_name:20} {val1:^25.3f} â”‚ {val2:^25.3f} â”‚ {arrow} {color_diff}")
    
    print("=" * 80)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--compare" and len(sys.argv) >= 4:
            # ë¹„êµ ëª¨ë“œ
            compare_results(sys.argv[2], sys.argv[3])
        else:
            # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
            from visualize_results import main as visualize_main
            sys.argv = ['visualize_results.py'] + sys.argv[1:]
            visualize_main()
    else:
        # ëŒ€ì‹œë³´ë“œ ëª¨ë“œ
        dashboard = Dashboard()
        dashboard.run_interactive()